{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本部分代码同DFCNN.ipynb，仅仅是读取label时读取拼音而非中文，故不多加注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#相关包\n",
    "from scipy.io import wavfile #音频数据读取\n",
    "from scipy.fftpack import fft #音频数据处理-傅里叶变换\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import shuffle #数据生成器中的打乱\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D\n",
    "from keras.layers import Reshape, Dense, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本部分自定义函数：\n",
    "# 从文件夹中读取音频数据、标签的文件名，存入数组\n",
    "def source_get(source_file):\n",
    "    train_file = source_file + '/train'\n",
    "    label_lst = []\n",
    "    wav_lst = []\n",
    "    for root, dirs, files in os.walk(train_file):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav') or file.endswith('.WAV'):\n",
    "                wav_file = os.sep.join([root, file])\n",
    "                label_file = source_file + '/data/' + file +'.trn'\n",
    "                wav_lst.append(wav_file)\n",
    "                label_lst.append(label_file)\n",
    "\n",
    "    return label_lst, wav_lst\n",
    "\n",
    "# 输入路径，读取音频文件对应的label\n",
    "def read_label(label_file):\n",
    "    with open(label_file, 'r', encoding='utf8') as f:\n",
    "        data = f.readlines()\n",
    "        return data[1]\n",
    "\n",
    "# 输入路径数组，读取所有label\n",
    "def gen_label_data(label_lst):\n",
    "    label_data = []\n",
    "    for label_file in label_lst:\n",
    "        pny = read_label(label_file)\n",
    "        label_data.append(pny.strip('\\n'))\n",
    "    return label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本部分自定义函数：\n",
    "# 提取本数据集label中所有汉字（不重复）形成词汇表vacab\n",
    "def mk_vocab(label_data):\n",
    "    vocab = []\n",
    "    for line in label_data:\n",
    "        line = line.split(' ')\n",
    "        for pny in line:\n",
    "            if pny not in vocab:\n",
    "                vocab.append(pny)\n",
    "    vocab.append('_')\n",
    "    return vocab\n",
    "\n",
    "# 将vocab中的汉字转数字id\n",
    "def word2id(line, vocab):\n",
    "    return [vocab.index(pny) for pny in line.split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#本部分自定义函数（同演示部分的函数）\n",
    "def compute_fbank(file):\n",
    "    x = np.linspace(0, 400 - 1, 400, dtype=np.int64)\n",
    "    w = 0.54 - 0.46 * np.cos(2 * np.pi * (x) / (400 - 1))  # 汉明窗\n",
    "    \n",
    "    sample_rate,wavdata = wavfile.read(file)  ##单声道是一维的，立体音是二维\n",
    "    \n",
    "    # wav波形 加时间窗以及时移10ms\n",
    "    time_window = 25  # 单位ms\n",
    "    window_length = sample_rate / 1000 * time_window  # 计算窗长度的公式，目前全部为400固定值\n",
    "    \n",
    "    wav_arr = np.array(wavdata)\n",
    "    wav_length = len(wavdata)\n",
    "    range0_end = int(len(wavdata) / sample_rate * 1000 - time_window) // 10  # 计算循环终止的位置，也就是最终生成的窗数\n",
    "    #print(range0_end)\n",
    "    \n",
    "    data_input = np.zeros((range0_end, 200), dtype=np.float64)  # 用于存放最终的频率特征数据\n",
    "    data_line = np.zeros((1, 400), dtype=np.float64)\n",
    "    for i in range(0, range0_end):\n",
    "        p_start = i * 160\n",
    "        p_end = p_start + 400\n",
    "        data_line = wav_arr[p_start:p_end]\n",
    "        data_line = data_line * w\n",
    "        data_line = np.abs(fft(data_line))\n",
    "        data_input[i] = data_line[0:200]\n",
    "    data_input = np.log(data_input + 1)\n",
    "    return data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本部分自定义函数：\n",
    "\n",
    "# 时频图的补零处理：\n",
    "def wav_padding(wav_data_lst):\n",
    "    \n",
    "    wav_lens = [len(data) for data in wav_data_lst] # 时频图长度\n",
    "    wav_max_len = max(wav_lens) # 时频图长度最大值\n",
    "    \n",
    "    #新的时频图矩阵（四维：前两维为图片个数、音频长度，后两维代表了每帧的时频图）\n",
    "    new_wav_data_lst = np.zeros((len(wav_data_lst), wav_max_len, 200, 1))\n",
    "    for i in range(len(wav_data_lst)):\n",
    "        new_wav_data_lst[i, :wav_data_lst[i].shape[0], :, 0] = wav_data_lst[i]\n",
    "    \n",
    "    #后续输入ctc的音频长度（由于ctc处理之前经过神经网络处理，长度缩短为8倍）\n",
    "    wav_lens = np.array([leng // 8 for leng in wav_lens])\n",
    "    \n",
    "    return new_wav_data_lst, wav_lens\n",
    "\n",
    "# label的补零处理：\n",
    "def label_padding(label_data_lst):\n",
    "    \n",
    "    label_lens = np.array([len(label) for label in label_data_lst])\n",
    "    max_label_len = max(label_lens)\n",
    "    \n",
    "    new_label_data_lst = np.zeros((len(label_data_lst), max_label_len))\n",
    "    for i in range(len(label_data_lst)):\n",
    "        new_label_data_lst[i][:len(label_data_lst[i])] = label_data_lst[i]\n",
    "        \n",
    "    return new_label_data_lst, label_lens\n",
    "\n",
    "# 用于训练格式的数据生成器\n",
    "def data_generator(batch_size, shuffle_list, wav_lst, label_data, vocab):\n",
    "    \n",
    "    for i in range(len(wav_lst) // batch_size):\n",
    "        wav_data_lst = []\n",
    "        label_data_lst = []\n",
    "        \n",
    "        begin = i * batch_size\n",
    "        end = begin + batch_size\n",
    "        sub_list = shuffle_list[begin:end]\n",
    "        \n",
    "        for index in sub_list:\n",
    "            #转为频谱图\n",
    "            fbank = compute_fbank(wav_lst[index])\n",
    "            pad_fbank = np.zeros((fbank.shape[0] // 8 * 8 + 8, fbank.shape[1]))\n",
    "            pad_fbank[:fbank.shape[0], :] = fbank\n",
    "            #读取label\n",
    "            label = word2id(label_data[index], vocab)\n",
    "            \n",
    "            wav_data_lst.append(pad_fbank)\n",
    "            label_data_lst.append(label)\n",
    "            \n",
    "        pad_wav_data, input_length = wav_padding(wav_data_lst)\n",
    "        pad_label_data, label_length = label_padding(label_data_lst)\n",
    "        \n",
    "        inputs = {'the_inputs': pad_wav_data,\n",
    "                  'the_labels': pad_label_data,\n",
    "                  'input_length': input_length,\n",
    "                  'label_length': label_length,\n",
    "                  }\n",
    "        outputs = {'ctc': np.zeros(pad_wav_data.shape[0], )}\n",
    "        \n",
    "        yield inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(units, activation=\"relu\"):\n",
    "    return Dense(units, activation='relu', use_bias=True,\n",
    "                 kernel_initializer='he_normal')\n",
    "\n",
    "\n",
    "#构造DFCNN中的卷积池化层函数（两个卷积+BN）\n",
    "def cnn_cell(size, x, pool=True):\n",
    "    x = Conv2D(size, (3, 3), use_bias=True, activation='relu',padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    x = Conv2D(size, (3, 3), use_bias=True, activation='relu',padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization(axis=-1)(x)\n",
    "    if pool:\n",
    "        x = MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(x)\n",
    "    return x\n",
    "\n",
    "#使用CTC损失函数（keras内置）\n",
    "def ctc_lambda(args):\n",
    "    labels, y_pred, input_length, label_length = args\n",
    "    y_pred = y_pred[:, :, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "\n",
    "#DFCNN模型\n",
    "class Amodel():\n",
    "    #给出必要的参数并对模型初始化\n",
    "    def __init__(self, vocab_size):\n",
    "        super(Amodel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self._model_init()\n",
    "        self._ctc_init()\n",
    "        self.opt_init()\n",
    "\n",
    "    #DFCNN构建（四个卷积池化层、两个全连接层）\n",
    "    def _model_init(self):\n",
    "        self.inputs = Input(name='the_inputs', shape=(None, 200, 1))\n",
    "        self.h1 = cnn_cell(32, self.inputs)\n",
    "        self.h2 = cnn_cell(64, self.h1)\n",
    "        self.h3 = cnn_cell(128, self.h2)\n",
    "        self.h4 = cnn_cell(128, self.h3, pool=False)\n",
    "        self.h6 = Reshape((-1, 3200))(self.h4)\n",
    "        self.h7 = Dense(256, activation='relu', use_bias=True, kernel_initializer='he_normal')(self.h6)\n",
    "        self.outputs = Dense(self.vocab_size, activation='softmax', use_bias=True, kernel_initializer='he_normal')(self.h7)\n",
    "        self.model = Model(inputs=self.inputs, outputs=self.outputs)\n",
    "    \n",
    "    #CTC构建（输入输出为前文padding函数的返回值）\n",
    "    def _ctc_init(self):\n",
    "        self.labels = Input(name='the_labels', shape=[None], dtype='float32')\n",
    "        self.input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "        self.label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "        self.loss_out = Lambda(ctc_lambda, output_shape=(1,), name='ctc') \\\n",
    "            ([self.labels, self.outputs, self.input_length, self.label_length])\n",
    "        self.ctc_model = Model(inputs=[self.labels, self.inputs,\n",
    "                                       self.input_length, self.label_length], outputs=self.loss_out)\n",
    "        \n",
    "    #优化函数构建（使用adam优化器）\n",
    "    def opt_init(self):\n",
    "        opt = Adam(lr=0.0008, beta_1=0.9, beta_2=0.999, decay=0.01, epsilon=10e-8)\n",
    "        self.ctc_model.compile(loss={'ctc': lambda y_true, output: output}, optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168\n"
     ]
    }
   ],
   "source": [
    "total_nums = 2000\n",
    "batch_size = 8 # batch_size太大会出现内存溢出\n",
    "batch_num = total_nums // batch_size\n",
    "epochs = 50\n",
    "\n",
    "source_file = '/mnt/thchs30'\n",
    "label_lst, wav_lst = source_get(source_file)\n",
    "label_data = gen_label_data(label_lst[:total_nums])\n",
    "vocab = mk_vocab(label_data)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab_size)\n",
    "\n",
    "shuffle_list = [i for i in range(total_nums)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_inputs (InputLayer)         (None, None, 200, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, 200, 32 320         the_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 200, 32 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, 200, 32 9248        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 200, 32 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, None, 100, 32 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, 100, 64 18496       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 100, 64 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, 100, 64 36928       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, 100, 64 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, None, 50, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, 50, 128 73856       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, 50, 128 512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, 50, 128 147584      batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, 50, 128 512         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, None, 25, 128 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, 25, 128 147584      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, 25, 128 512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, 25, 128 147584      batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, 25, 128 512         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, None, 3200)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 256)    819456      reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "the_labels (InputLayer)         (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 1168)   300176      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "label_length (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "ctc (Lambda)                    (None, 1)            0           the_labels[0][0]                 \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 input_length[0][0]               \n",
      "                                                                 label_length[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,704,048\n",
      "Trainable params: 1,702,640\n",
      "Non-trainable params: 1,408\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "am = Amodel(vocab_size)\n",
    "am.ctc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the 1 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 433s 2s/step - loss: 222.6170\n",
      "this is the 2 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 243s 972ms/step - loss: 204.3487\n",
      "this is the 3 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 243s 972ms/step - loss: 183.9723\n",
      "this is the 4 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 243s 971ms/step - loss: 153.4364\n",
      "this is the 5 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 129.2037\n",
      "this is the 6 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 970ms/step - loss: 112.3276\n",
      "this is the 7 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 243s 972ms/step - loss: 100.4432\n",
      "this is the 8 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 243s 972ms/step - loss: 91.6195\n",
      "this is the 9 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 243s 973ms/step - loss: 84.7561\n",
      "this is the 10 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 243s 973ms/step - loss: 79.2035\n",
      "this is the 11 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 243s 972ms/step - loss: 74.5832\n",
      "this is the 12 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 243s 973ms/step - loss: 70.6456\n",
      "this is the 13 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 970ms/step - loss: 67.2442\n",
      "this is the 14 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 968ms/step - loss: 64.2445\n",
      "this is the 15 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 61.5824\n",
      "this is the 16 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 968ms/step - loss: 59.1930\n",
      "this is the 17 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 57.0308\n",
      "this is the 18 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 55.0567\n",
      "this is the 19 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 53.2432\n",
      "this is the 20 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 966ms/step - loss: 51.5654\n",
      "this is the 21 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 50.0077\n",
      "this is the 22 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 968ms/step - loss: 48.5571\n",
      "this is the 23 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 969ms/step - loss: 47.2011\n",
      "this is the 24 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 45.9282\n",
      "this is the 25 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 44.7304\n",
      "this is the 26 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 966ms/step - loss: 43.5980\n",
      "this is the 27 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 968ms/step - loss: 42.5264\n",
      "this is the 28 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 968ms/step - loss: 41.5100\n",
      "this is the 29 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 966ms/step - loss: 40.5416\n",
      "this is the 30 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 968ms/step - loss: 39.6185\n",
      "this is the 31 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 38.7411\n",
      "this is the 32 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 37.9009\n",
      "this is the 33 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 37.0964\n",
      "this is the 34 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 968ms/step - loss: 36.3271\n",
      "this is the 35 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 243s 973ms/step - loss: 35.5863\n",
      "this is the 36 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 244s 974ms/step - loss: 34.8722\n",
      "this is the 37 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 970ms/step - loss: 34.1861\n",
      "this is the 38 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 243s 973ms/step - loss: 33.5263\n",
      "this is the 39 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 244s 974ms/step - loss: 32.8896\n",
      "this is the 40 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 243s 974ms/step - loss: 32.2751\n",
      "this is the 41 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 245s 982ms/step - loss: 31.6819\n",
      "this is the 42 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 244s 976ms/step - loss: 31.1067\n",
      "this is the 43 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 30.5522\n",
      "this is the 44 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 968ms/step - loss: 30.0142\n",
      "this is the 45 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 966ms/step - loss: 29.4939\n",
      "this is the 46 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 28.9878\n",
      "this is the 47 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 28.4963\n",
      "this is the 48 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 28.0194\n",
      "this is the 49 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 27.5567\n",
      "this is the 50 th epochs trainning !!!\n",
      "Epoch 1/1\n",
      "250/250 [==============================] - 242s 967ms/step - loss: 27.1059\n"
     ]
    }
   ],
   "source": [
    "loss = []\n",
    "for k in range(epochs):\n",
    "    print('this is the', k + 1, 'th epochs trainning !!!')\n",
    "    batch = data_generator(batch_size, shuffle_list, wav_lst, label_data, vocab)\n",
    "    result = am.ctc_model.fit_generator(batch, steps_per_epoch=batch_num, epochs=1)\n",
    "    loss.append(result.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'ctc-loss')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 560x320 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loss的可视化\n",
    "x = range(1,51)\n",
    "plt.figure(figsize=(7,4),dpi=80)\n",
    "plt.plot(x, loss)\n",
    "plt.xlabel('epochs') \n",
    "plt.ylabel('ctc-loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数字结果：  [  0 482   2   3   4   5   6   3 572   8   9  10  11  12  34  14  15  16\n",
      "  17  18  19  20  21  22  23  24  25  26  27  28]\n",
      "文本结果： ['zhu4', 'mian4', 'yan3', 'yuan2', 'you3', 'cheng2', 'fang1', 'yuan2', 'mu4', 'xuan4', 'fu3', 'yuan3', 'zheng1', 'zhang1', 'quan2', 'chen2', 'jun4', 'hua2', 'zi1', 'sha1', 'li4', 'ou1', 'yang2', 'bin1', 'lin2', 'wen2', 'zeng1', 'zhou1', 'wei2', 'deng3']\n",
      "原文结果： ['zhu4', 'ming2', 'yan3', 'yuan2', 'you3', 'cheng2', 'fang1', 'yuan2', 'mou2', 'xuan4', 'fu3', 'yuan3', 'zheng1', 'zhang1', 'qiang2', 'chen2', 'jun4', 'hua2', 'zi1', 'sha1', 'li4', 'ou1', 'yang2', 'bin1', 'lin2', 'wen2', 'zeng1', 'zhou1', 'wei2', 'deng3']\n",
      "数字结果：  [ 29  30  31  72 277  34  35  36 142  38  39 192  40  41 419 915  44   5\n",
      "  46  45  27  47 248  48 368  49  28]\n",
      "文本结果： ['ke3', 'fa1', 'zhan3', 'sheng1', 'nian2', 'quan2', 'se4', 'bei4', 'kan1', 'yin4', 'hua1', 'bi4', 'dan1', 'gou1', 'gan1', 'sun3', 'rong2', 'cheng2', 'zhao4', 'chuang2', 'wei2', 'zhen3', 'ta1', 'pei4', 'fen1', 'jian4', 'deng3']\n",
      "原文结果： ['ke3', 'fa1', 'zhan3', 'chun2', 'mian2', 'quan2', 'se4', 'bei4', 'tao4', 'yin4', 'hua1', 'bei4', 'dan1', 'gou1', 'bian1', 'cu4', 'rong2', 'chuang2', 'zhao4', 'chuang2', 'wei2', 'zhen3', 'tao4', 'pei4', 'tao4', 'jian4', 'deng3']\n",
      "数字结果：  [  0  50  42  51 304  53 895  55  56  57 513 114  56 122  60  65 324  63\n",
      "  64 482  98 280  67  68  53 280   4  70  71  72]\n",
      "文本结果： ['zhu4', 'zang4', 'bian1', 'fang2', 'mu3', 'bu4', 'huai4', 'lian2', 'zhan4', 'shi4', 'gai4', 'jun1', 'zhan4', 'ku3', 'shi2', 'ying2', 'yun2', 'yue1', 'ting1', 'mian4', 'yan2', 'chu1', 'wai4', 'de5', 'bu4', 'chu1', 'you3', 'ku1', 'qi4', 'sheng1']\n",
      "原文结果： ['zhu4', 'zang4', 'bian1', 'fang2', 'mou3', 'bu4', 'er4', 'lian2', 'zhan4', 'shi4', 'zhao4', 'jin1', 'zhan4', 'gang3', 'shi2', 'yin2', 'yin3', 'yue1', 'yue1', 'ting1', 'jian4', 'ying2', 'qu1', 'wai4', 'de5', 'bu4', 'yuan3', 'chu4', 'you3', 'ku1', 'qi4', 'sheng1']\n",
      "数字结果：  [ 73  74  67  75   4  76   6  95 903  79 125  28  80  81 456  82  83  84\n",
      "  60  85  76  83  86  87  88  89   5  90  91  92]\n",
      "文本结果： ['qiong2', 'long2', 'wai4', 'xing2', 'you3', 'si4', 'fang1', 'yong4', 'mao2', 'ba1', 'neng2', 'deng3', 'gao1', 'da2', 'you4', 'qi1', 'ceng2', 'zhi4', 'shi2', 'san1', 'si4', 'ceng2', 'xia4', 'kuan1', 'shang4', 'zhai3', 'cheng2', 'jie1', 'ti1', 'zhuang4']\n",
      "原文结果： ['qiong2', 'long2', 'wai4', 'xing2', 'you3', 'si4', 'fang1', 'liu4', 'leng2', 'ba1', 'leng2', 'deng3', 'gao1', 'da2', 'liu4', 'qi1', 'ceng2', 'zhi4', 'shi2', 'san1', 'si4', 'ceng2', 'xia4', 'kuan1', 'shang4', 'zhai3', 'cheng2', 'jie1', 'ti1', 'zhuang4']\n",
      "数字结果：  [ 64  68  93  94  36  96  53 331  68  98  98  99 193  64 100 101  24  74\n",
      " 350 851 752 106 260 108 109 673 236 241]\n",
      "文本结果： ['ting1', 'de5', 'chuang1', 'kou3', 'bei4', 'hei1', 'bu4', 'zhe4', 'de5', 'yan2', 'yan2', 'shi5', 'shi3', 'ting1', 'nei4', 'hun1', 'wen2', 'long2', 'wei4', 'qing3', 'chong1', 'hong2', 'li3', 'deng1', 'pin2', 'ping2', 'shen2', 'shuo1']\n",
      "原文结果： ['ting1', 'de5', 'chuang1', 'kou3', 'bei4', 'yong4', 'hei1', 'bu4', 'zhe1', 'de5', 'yan2', 'yan2', 'shi5', 'shi5', 'ting1', 'nei4', 'hun1', 'an4', 'meng2', 'long2', 'wei2', 'you3', 'ji3', 'chuan4', 'hong2', 'lv4', 'deng1', 'pin2', 'pin5', 'shan3', 'shuo4']\n",
      "数字结果：  [206 255 115 113 116  52  53 117 118  75   3 119 243  29 122 123  98 118\n",
      "  75 124 125 126 127 129  34 131 139 132 118  75   3 115 133]\n",
      "文本结果： ['hu2', 'jue2', 'hang2', 'kong1', 'bing1', 'mou3', 'bu4', 'nv3', 'fei1', 'xing2', 'yuan2', 'wang2', 'qi2', 'ke3', 'ku3', 'zuan1', 'yan2', 'fei1', 'xing2', 'ji4', 'neng2', 'zui4', 'jin4', 'ru4', 'quan2', 'tian1', 'ying1', 'hou4', 'fei1', 'xing2', 'yuan2', 'hang2', 'lie4']\n",
      "原文结果： ['kong1', 'jun1', 'hang2', 'kong1', 'bing1', 'mou3', 'bu4', 'nv3', 'fei1', 'xing2', 'yuan2', 'wang2', 'qin2', 'ke4', 'ku3', 'zuan1', 'yan2', 'fei1', 'xing2', 'ji4', 'neng2', 'zui4', 'jin4', 'kua4', 'ru4', 'le5', 'quan2', 'tian1', 'hou4', 'fei1', 'xing2', 'yuan2', 'hang2', 'lie4']\n",
      "数字结果：  [  53  134   20   26   34  135  136  137  138   84  139  140  141   50\n",
      "  749   84   34   24 1005  285  130   12  143   68  144  145  146   24]\n",
      "文本结果： ['bu4', 'jiu3', 'ou1', 'zhou1', 'quan2', 'wei1', 'yi1', 'xue2', 'za2', 'zhi4', 'ying1', 'guo2', 'xin1', 'zang4', 'fan2', 'zhi4', 'quan2', 'wen2', 'tan1', 'de2', 'le5', 'zhang1', 'yun4', 'de5', 'liang3', 'pian1', 'lun4', 'wen2']\n",
      "原文结果： ['bu4', 'jiu3', 'ou1', 'zhou1', 'quan2', 'wei1', 'yi1', 'xue2', 'za2', 'zhi4', 'ying1', 'guo2', 'xin1', 'zang4', 'za2', 'zhi4', 'quan2', 'wen2', 'kan1', 'deng1', 'le5', 'zhang1', 'yun4', 'de5', 'liang3', 'pian1', 'lun4', 'wen2']\n",
      "数字结果：  [147 569 149 150 151  57 102 153  83  57  79 154 114 136  54  60 134 156\n",
      " 157 158  53 282 115 114  66 157 382  53 147 159 115  82  79 161  68   0\n",
      " 162]\n",
      "文本结果： ['he2', 'ban1', 'sheng3', 'she4', 'xian4', 'shi4', 'an4', 'cun1', 'ceng2', 'shi4', 'ba1', 'lu4', 'jun1', 'yi1', 'er4', 'shi2', 'jiu3', 'shi1', 'si1', 'ling4', 'bu4', 'zai4', 'hang2', 'jun1', 'qu1', 'si1', 'mei4', 'bu4', 'he2', 'tai4', 'hang2', 'qi1', 'ba1', 'wei3', 'de5', 'zhu4', 'di4']\n",
      "原文结果： ['he2', 'bei2', 'sheng3', 'she4', 'xian4', 'chi4', 'an4', 'cun1', 'ceng2', 'shi4', 'ba1', 'lu4', 'jun1', 'yi2', 'er4', 'jiu3', 'shi1', 'si1', 'ling4', 'bu4', 'tai4', 'hang2', 'jun1', 'qu1', 'si1', 'ling4', 'bu4', 'he2', 'tai4', 'hang2', 'qu1', 'dang2', 'wei3', 'de5', 'zhu4', 'di4']\n",
      "数字结果：  [163 164  60 165 166  88 167  21 162  57 168 169 170 171   3 247  21  57\n",
      " 172  27 120 173  66 167  21 162  66 172 350 167  21  57]\n",
      "文本结果： ['er2', 'ci3', 'shi2', 'zheng4', 'gan3', 'shang4', 'xian2', 'yang2', 'di4', 'shi4', 'ji1', 'gou4', 'bian4', 'hua4', 'yuan2', 'qie3', 'yang2', 'shi4', 'gai3', 'wei2', 'qin2', 'du1', 'qu1', 'xian2', 'yang2', 'di4', 'qu1', 'gai3', 'wei4', 'xian2', 'yang2', 'shi4']\n",
      "原文结果： ['er2', 'ci3', 'shi2', 'zheng4', 'gan3', 'shang4', 'xian2', 'yang2', 'di4', 'shi4', 'ji1', 'gou4', 'bian4', 'hua4', 'yuan2', 'xian2', 'yang2', 'shi4', 'gai3', 'wei2', 'qin2', 'du1', 'qu1', 'xian2', 'yang2', 'di4', 'qu1', 'gai3', 'wei2', 'xian2', 'yang2', 'shi4']\n",
      "数字结果：  [174  57 175 176 177  29 125  57 178 179 180 181 210  80 183 179 184 336\n",
      "  68 143 186   3 187 188  57   4 189 181  68 190]\n",
      "文本结果： ['bu2', 'shi4', 'chui1', 'niu2', 'ye3', 'ke3', 'neng2', 'shi4', 'zi4', 'wo3', 'yao1', 'qiu2', 'jiao3', 'gao1', 'yu3', 'wo3', 'xiang1', 'shu1', 'de5', 'yun4', 'dong4', 'yuan2', 'da4', 'duo1', 'shi4', 'you3', 'zhui1', 'qiu2', 'de5', 'ren2']\n",
      "原文结果： ['bu2', 'shi4', 'chui1', 'niu2', 'ye3', 'ke3', 'neng2', 'shi4', 'zi4', 'wo3', 'yao1', 'qiu2', 'jiao4', 'gao1', 'yu3', 'wo3', 'xiang1', 'shu2', 'de5', 'yun4', 'dong4', 'yuan2', 'da4', 'duo1', 'shi4', 'you3', 'zhui1', 'qiu2', 'de5', 'ren2']\n"
     ]
    }
   ],
   "source": [
    "#对模型输出结果进行解码，转为拼音\n",
    "def decode_ctc(num_result, num2word):\n",
    "    result = num_result[:, :, :]\n",
    "    in_len = np.zeros((1), dtype=np.int32)\n",
    "    in_len[0] = result.shape[1]\n",
    "    r = K.ctc_decode(result, in_len, greedy=True, beam_width=10, top_paths=1)\n",
    "    r1 = K.get_value(r[0][0])\n",
    "    r1 = r1[0]\n",
    "    text = []\n",
    "    for i in r1:\n",
    "        text.append(num2word[i])\n",
    "    return r1, text\n",
    "\n",
    "batch = data_generator(1, shuffle_list, wav_lst, label_data, vocab)\n",
    "for i in range(10):\n",
    "    # 载入训练好的模型，并进行识别\n",
    "    inputs, outputs = next(batch)\n",
    "    x = inputs['the_inputs']\n",
    "    y = inputs['the_labels'][0]\n",
    "    result = am.model.predict(x, steps=1)\n",
    "    # 将数字结果转化为文本结果\n",
    "    result, text = decode_ctc(result, vocab)\n",
    "    print('数字结果： ', result)\n",
    "    print('文本结果：', text)\n",
    "    print('原文结果：', [vocab[int(i)] for i in y])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
